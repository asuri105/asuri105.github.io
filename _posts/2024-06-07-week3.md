---
layout: post
title: Week 3
---

Week 3 followed a similar progression to Week 2; it was made up of creating autograders, providing peer code reviews, and reading research papers on autograders and open-ended assignments. This week I was able to finish up my first solo autorgrader, using peer and mentor feedback to improve upon my test cases and example student solutions. The autograder I wrote was for a lab that had students create art work using while loops, user input, and indexing. During this work, I took some time to learn more about git, github, and how to commit changes to different branches. I also provided more peer code review on the autograders created by both of my undergraduate peers working on the project with me. In addition, I read two more research papers, highlighted below, that helped give me a better sense of why it is important to provide students meaningful feedback in our test cases and how that can impact student motivation. This week, the research class focused on data visualization, giving good presentations, and support systems, and as a part of class, we continued to work on our research proposals for our summer projects. And I ended the week by teaming up with my peers to work on a new autograder together for a lab on debugging. 


**Papers Read:**

*"How to Catch Novice Programmers' Struggles: Detecting Moments of Struggle in Open-Ended Block-Based Programming Projects using Trace Log Data" by Benyamin Tabarsi, Ally Limke, Heidi Reichert, Rachel Qualls, Thomas Price, Chris Martens, and Tiffany Barnes*

With computer science enrollment on the rise and the subject having a learning curve, it is becoming ever so important to be able to detect when students begin to struggle to ensure that they can be helped before they lose motivation and interest in the subject. This paper describes a study of using students perceptions of their progress and their Snap! trace data to create detectors that can detect certain student actions and know they correlate to what students are feeling. The hope was that these detectors could help instructors identify when to intervine so that students don't lose too much motivation when they have trouble with their programs.

*"Learning with Style: Improving Student Code-Style Through Better Automated Feedback" by Liam Saliba, Elisa Shioji, Eduardo Oliveira, Shaanan Cohney, and Jianzhong Qi*

Autograders are being introduced more rapidly in computer science courses to help alleviate workload from professors and TAs; however, they can be very inflexible and not very helpful in helping students understand where exactly they went wrong in their code. This paper presents an autograder, called ccheck, designed to provide more constructive feedback to students that can actually help them improve upon their code and learn side by side.
